{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Tracking the COVID-19 outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "This is a companion notebook for the dashboard web application.\n",
    "\n",
    "It provides transparency on the code used to source, prepare and visualise the data.\n",
    "\n",
    "Although the app1.py file contains all of this, the majority of the code in that file is specific to defining the set-up, styling and dynamic behaviour of the web application itself. \n",
    "\n",
    "This notebook is relevant to audiences interested in the content itself.\n",
    "\n",
    "For more information on building interactive data-driven dashboards in Python (and only Python, no JavaScript here!), I highly recommend looking at <a href=https://plotly.com/dash/> Plotly Dash </a> \n",
    "\n",
    "The dashboard is accessible at http://covid-ww.herokuapp.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on streaming vs manual data sourcing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier versions of this app streamed data direct from the relevant APIs. This meant each call to the website would triggered a refresh of 7 distinct data calls, resulting in longer load-time and frequent request time-out errors. \n",
    "\n",
    "Since each dataset is refreshed once every 24 hours, I decided to store static versions of all the relevant data locally, and perform a parallel job each morning to refresh all of the data at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import requests\n",
    "import datetime as dt\n",
    "from plotly.subplots import make_subplots\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up chart layouts\n",
    "\n",
    "l_map=go.Layout(\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "    geo={\n",
    "    'visible': True, \n",
    "    'resolution':50, \n",
    "    'showcountries':True, \n",
    "    'countrycolor':\"grey\",\n",
    "    'showsubunits':True, \n",
    "    'subunitcolor':\"White\",\n",
    "    'showframe':False,\n",
    "    'coastlinecolor':\"slategrey\",\n",
    "    'countrycolor':'white',\n",
    "    }\n",
    ")\n",
    "\n",
    "#UK Map\n",
    "l_mapbox=go.Layout(\n",
    "  margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "  mapbox_style=\"carto-positron\",\n",
    "  mapbox={\n",
    "    'bearing':0,\n",
    "    'center':go.layout.mapbox.Center(lat=55.3781,lon=-3.6360),\n",
    "    'pitch':0,\n",
    "    'zoom':4\n",
    "    },\n",
    "  \n",
    " )\n",
    "\n",
    "#HEATMAP & LINE CHART\n",
    "l_trend=go.Layout(\n",
    "  margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
    "  template=\"plotly_white\",\n",
    "  yaxis={\"tickfont\":{\"size\":10}},\n",
    "  xaxis={\"tickfont\":{\"size\":10}},\n",
    "  legend={'x':0.01, 'y':0.98, 'font':{'size':10}}\n",
    ")\n",
    "\n",
    "#SIMPLE BARS\n",
    "l_bar_s=go.Layout(\n",
    "  height=180,\n",
    "  margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "  template=\"plotly_white\",\n",
    "  yaxis={\"tickfont\":{\"size\":9}},\n",
    "  xaxis={\"tickfont\":{\"size\":9}},\n",
    "  legend={'x':0.02, 'y':0.96, 'font':{'size':9}}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Worldwide statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Data Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    " - **Cases and deaths** timeseries data: <a href=https://github.com/CSSEGISandData> Johns Hopkins Universtity Centre for Systems Science and Engineering </a> \n",
    " - **World Population by Country**:  <a href=https://population.un.org/wpp/Download/Standard/CSV/> UN World Population Prospects 2019 </a> \n",
    " - **Lat/Long co-ordinates** for world countries: <a href=https://developers.google.com/public-data/docs/canonical/countries_csv> Google </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Import of cases, deaths and world population data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#import timeseries data for cases (url_1) and deaths (url_2)\n",
    "url_1='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "url_2='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "df1=pd.read_csv(url_1)\n",
    "df2=pd.read_csv(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File countries.csv does not exist: 'countries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3e55b4a6c701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Read helper files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mctrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'countries.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcontinents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"continents.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"worldpop.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Location\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File countries.csv does not exist: 'countries.csv'"
     ]
    }
   ],
   "source": [
    "#Read helper files\n",
    "ctrs=pd.read_csv('countries.csv').set_index(\"name\")\n",
    "continents=pd.read_csv(\"continents.csv\").drop(columns=\"Unnamed: 0\")\n",
    "df_p=pd.read_csv(\"worldpop.csv\").set_index(\"Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data transformations \n",
    "\n",
    "def prep_world_data(df):\n",
    "    df=df.groupby(by=\"Country/Region\").sum()\n",
    "    df.drop(labels=[\"Lat\", \"Long\"], axis=1, inplace=True)\n",
    "    df.loc[:,:str(df.columns[-1])]=np.clip(df.loc[:,:str(df.columns[-1])], a_min=0, a_max=None)\n",
    "    df.rename(index={\"US\": 'United States of America'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def prep_countries_data(d1,d2):\n",
    "    d1=prep_world_data(d1)\n",
    "    d2=prep_world_data(d2)\n",
    "    countries=ctrs.merge(d1[d1.columns[-1]], left_index=True, right_index=True)\n",
    "    countries.rename(columns={d1.columns[-1]: \"Confirmed\"}, inplace=True)\n",
    "    countries=countries.merge(d2[d2.columns[-1]], left_index=True, right_index=True)\n",
    "    countries.rename(columns={d2.columns[-1]: \"Deaths\"}, inplace=True)\n",
    "    countries=countries.merge(df_p[\"PopTotal\"], left_index=True, right_index=True)\n",
    "    countries[\"Conf100\"]=round(countries[\"Confirmed\"]*100/countries[\"PopTotal\"])\n",
    "    countries[\"Deaths100\"]=round(countries[\"Deaths\"]*100/countries[\"PopTotal\"])\n",
    "    countries[\"Mortality\"]=round(countries[\"Deaths\"]*100/countries[\"Confirmed\"],2)\n",
    "    countries[\"text\"]= countries.index +'<br>' + \"Cases: \" + countries[\"Confirmed\"].astype('str') + '<br>' + \"Deaths: \"+ countries[\"Deaths\"].astype('str')\n",
    "    countries[\"text100\"]= countries.index +'<br>' + \"Cases per 100k pop.: \" + countries[\"Conf100\"].astype('str') + '<br>' + \"Deaths per 100k pop.: \"+ countries[\"Deaths100\"].astype('str')\n",
    "    return countries\n",
    "\n",
    "def prep_world_trend(d):\n",
    "    d=prep_world_data(d)\n",
    "    world_trend=d.merge(continents, left_index=True, right_on=\"Country\").groupby(\"Continent\").sum()\n",
    "    return world_trend\n",
    "\n",
    "def prep_world_newcases(d):\n",
    "    df=prep_world_trend(d)\n",
    "    world_newcases=pd.DataFrame(index=df.index,columns=list(df.columns[1:]))\n",
    "    for i in range(len(df.columns)-1):\n",
    "        f=df[df.columns[i+1]]-df[df.columns[i]]\n",
    "        world_newcases[world_newcases.columns[i]]=f\n",
    "    return world_newcases\n",
    "\n",
    "def prep_world_capita (d):\n",
    "    d1=prep_world_data(d)\n",
    "    t=d1.merge(df_p, left_index=True, right_index=True)\n",
    "    a=t.loc[:, :d1.columns[-1]].values.transpose()\n",
    "    b=100/t[\"PopTotal\"].values\n",
    "    c=a*b\n",
    "    t.loc[:, :d1.columns[-1]]=c.transpose()\n",
    "    return t\n",
    "\n",
    "def prep_rolling_avg(d):\n",
    "    d1=prep_world_capita(d)\n",
    "    d11=prep_world_data(d)\n",
    "    sr_5d=pd.DataFrame(index=d1.index,columns=list(d11.columns[5:]))\n",
    "    for i in range(len(d11.columns)-5):\n",
    "        f=(d1[d1.columns[i+5]]-d1[d1.columns[i]])/5\n",
    "        sr_5d[sr_5d.columns[i]]=f\n",
    "    return sr_5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Define helper charts (RHS OF WORLD MAP)\n",
    "\n",
    "#Country top 15's\n",
    "def make_fig_2_3(d1,d2,text):\n",
    "    fig = go.Figure(go.Bar(y=prep_countries_data(d1,d2).sort_values(by=[text], ascending=True).index[-15:],\n",
    "                          x=prep_countries_data(d1,d2).sort_values(by=[text],ascending=True)[text][-15:],\n",
    "                         orientation='h'\n",
    "                        )\n",
    "                 )\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "#Cumulative cases/deaths by continent\n",
    "\n",
    "def make_fig_4(d):\n",
    "    fig = go.Figure(data=[go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[0],\n",
    "                          name=prep_world_trend(d).index[0]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[1],\n",
    "                          name=prep_world_trend(d).index[1]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[2],\n",
    "                          name=prep_world_trend(d).index[2]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[3],\n",
    "                          name=prep_world_trend(d).index[3]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[4],\n",
    "                          name=prep_world_trend(d).index[4]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_trend(d).columns)),\n",
    "                          y=prep_world_trend(d).iloc[5],\n",
    "                          name=prep_world_trend(d).index[5])]\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "\n",
    "#New cases/deaths by continent\n",
    "def make_fig_5(d):\n",
    "    fig = go.Figure(data=[go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[0],\n",
    "                          name=prep_world_newcases(d).index[0]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[1],\n",
    "                          name=prep_world_newcases(d).index[1]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[2],\n",
    "                          name=prep_world_newcases(d).index[2]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[3],\n",
    "                          name=prep_world_newcases(d).index[3]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[4],\n",
    "                          name=prep_world_newcases(d).index[4]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_world_newcases(d).columns)),\n",
    "                          y=prep_world_newcases(d).iloc[5],\n",
    "                          name=prep_world_newcases(d).index[5]),\n",
    "                        ]\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**World map and Worst-hit countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = go.Figure(go.Scattergeo(\n",
    "    lon=prep_countries_data(df1,df2)[\"longitude\"],\n",
    "    lat=prep_countries_data(df1,df2)[\"latitude\"],\n",
    "    text = prep_countries_data(df1,df2)['text100'],\n",
    "    hoverinfo = 'text',\n",
    "    marker=dict(\n",
    "        size= 0.5*prep_countries_data(df1,df2)[\"Conf100\"],\n",
    "        line_width=0.5,\n",
    "        sizemode='area'\n",
    "    )))\n",
    "fig1=go.Figure(data=d)\n",
    "fig1.update_layout(l_map)\n",
    "fig1.update_geos(projection_type=\"natural earth\", lataxis_showgrid=False, lonaxis_showgrid=False)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_2_3(df1,df2,\"Confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_2_3(df1,df2,\"Conf100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_4(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_5(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[\"Switzerland\", \"United Kingdom\", \"Spain\", \"Italy\", \"France\"]\n",
    "d3=[{\n",
    "    'x': pd.to_datetime(np.array(prep_world_data(df1).columns)),\n",
    "    'y': prep_world_data(df1)[prep_world_data(df1).index==country].values[0],\n",
    "    'name': country\n",
    "    } for country in li]\n",
    "fig3=go.Figure(data=d3,layout=l_trend)\n",
    "#fig3.update_layout(yaxis_type=\"log\")\n",
    "fig3.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=go.Heatmap(\n",
    "        z=prep_rolling_avg(df2).loc[li],\n",
    "        x=pd.to_datetime(np.array(prep_rolling_avg(df2).columns)),\n",
    "        y=li,\n",
    "        colorscale='Blues',\n",
    "        colorbar={\"thickness\":10, \"tickfont\":{\"size\":10}},\n",
    "        ) \n",
    "fig4=go.Figure(data=data,layout=l_trend)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### USA statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Data Sources**\n",
    "\n",
    "- **Cases and Deaths**: <a href=https://github.com/CSSEGISandData> Johns Hopkins Universtity Centre for Systems Science and Engineering </a> \n",
    "- **Population by County**: <a href=https://www.census.gov/programs-surveys/popest/data/data-sets.html> United States Census Bureau </a>\n",
    "- **County co-ordinates, Regional state groupings**: Same source as Cases and Deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Data ingestion & tranformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "url_4='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "url_5='https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv'\n",
    "\n",
    "df3=pd.read_csv(url_3)\n",
    "df4=pd.read_csv(url_4).drop(\"Population\", axis=1)\n",
    "us_regions=pd.read_csv(url_5)\n",
    "us_pop=pd.read_csv('us_pop.csv')\n",
    "\n",
    "def prep_county_data(d):\n",
    "    df=d.drop(labels=[\"UID\",\"iso2\", \"iso3\",\"Province_State\",\"Country_Region\"], axis=1)\n",
    "    df.drop(df[df[\"Admin2\"].isnull()].index, inplace=True)\n",
    "    df.drop(df[df[\"FIPS\"].isnull()].index, inplace=True)\n",
    "    return df   \n",
    "\n",
    "def prep_state_data(d):\n",
    "    df=d.groupby(by=\"Province_State\").sum()\n",
    "    df.drop(labels=[\"UID\", \"code3\", \"FIPS\", \"Lat\", \"Long_\"], axis=1, inplace=True)\n",
    "    df.loc[:,:str(df.columns[-1])]=np.clip(df.loc[:,:str(df.columns[-1])], a_min=0, a_max=None)\n",
    "    return df\n",
    "\n",
    "def prep_county_sum(d1,d2):\n",
    "    dd1=prep_county_data(d1)\n",
    "    dd2=prep_county_data(d2)\n",
    "    df=dd1[[\"Admin2\", \"FIPS\", \"Lat\", \"Long_\", dd1.columns[-1]]]\n",
    "    df=df.rename(columns={dd1.columns[-1]: \"Confirmed\"})\n",
    "    df=df.merge(dd2[[\"FIPS\",dd2.columns[-1]]], on=\"FIPS\")\n",
    "    df=df.rename(columns={dd2.columns[-1]: \"Deaths\"})\n",
    "    df=df.merge(us_pop[[\"FIPS\", \"STNAME\",\"CTYNAME\", \"POPESTIMATE2019\", \"Abbreviation\"]], on=\"FIPS\")\n",
    "    df[\"Mortality\"]=round(df[\"Deaths\"]*100/df[\"Confirmed\"],2)\n",
    "    df[\"Conf100\"]=df[\"Confirmed\"]*100000/df[\"POPESTIMATE2019\"]\n",
    "    df[\"Deaths100\"]=df[\"Deaths\"]*100000/df[\"POPESTIMATE2019\"]\n",
    "    df[\"Ctylabel\"]=df[\"Admin2\"]+ \", \"+df[\"Abbreviation\"]\n",
    "    df[\"text\"]= df[\"Ctylabel\"] + '<br>Cases: ' + (round(df['Confirmed'],1).astype(str))+ '<br>Deaths: ' + (round(df['Deaths'],1).astype(str))\n",
    "    df[\"text100\"]= df[\"Ctylabel\"] + '<br>Cases per 100k pop: ' + (round(df['Conf100'],1).astype(str))+ '<br>Deaths per 100k pop: ' + (round(df['Deaths100'],1).astype(str))\n",
    "    return df\n",
    "\n",
    "def prep_state_sum(d1,d2):\n",
    "    df=prep_county_sum(d1,d2)\n",
    "    df=df[[\"STNAME\", \"Confirmed\", \"Deaths\", \"POPESTIMATE2019\"]].groupby(\"STNAME\").sum()\n",
    "    df[\"Conf100\"]=df[\"Confirmed\"]*100000/df[\"POPESTIMATE2019\"]\n",
    "    df[\"Deaths100\"]=df[\"Deaths\"]*100000/df[\"POPESTIMATE2019\"]\n",
    "    df[\"Mortality\"]=round(df[\"Deaths\"]*100/df[\"Confirmed\"],2)\n",
    "    return df\n",
    "\n",
    "def prep_us_trend(d):\n",
    "    df=prep_state_data(d)\n",
    "    df=df.merge(us_regions, left_index=True, right_on=\"State\")\n",
    "    df=df.groupby(\"Region\").sum()\n",
    "    return df\n",
    "\n",
    "def prep_us_newcases(d):\n",
    "    df=prep_us_trend(d)\n",
    "    dff=pd.DataFrame(index=df.index,columns=list(df.columns[1:]))\n",
    "    for i in range(len(df.columns)-1):\n",
    "        f=df[df.columns[i+1]]-df[df.columns[i]]\n",
    "        dff[dff.columns[i]]=f\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#County total cases\n",
    "def make_fig_7_8(d1,d2,text):\n",
    "    fig = go.Figure(go.Bar(y=prep_county_sum(d1,d2).sort_values(by=[text], ascending=True)[\"Ctylabel\"][-15:],\n",
    "                          x=prep_county_sum(d1,d2).sort_values(by=[text],ascending=True)[text][-15:],\n",
    "                         orientation='h'\n",
    "                        )\n",
    "                 )\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "#Region cululative cases\n",
    "\n",
    "def make_fig_9(d):\n",
    "    fig = go.Figure(data=[go.Bar(x=pd.to_datetime(np.array(prep_us_trend(d).columns)),\n",
    "                          y=prep_us_trend(d).iloc[0],\n",
    "                          name=prep_us_trend(d).index[0]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_trend(d).columns)),\n",
    "                          y=prep_us_trend(d).iloc[1],\n",
    "                          name=prep_us_trend(d).index[1]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_trend(d).columns)),\n",
    "                          y=prep_us_trend(d).iloc[2],\n",
    "                          name=prep_us_trend(d).index[2]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_trend(d).columns)),\n",
    "                          y=prep_us_trend(d).iloc[3],\n",
    "                          name=prep_us_trend(d).index[3])]\n",
    "\n",
    "                         #orientation='h'\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "#New cases by region\n",
    "\n",
    "def make_fig_10(d):\n",
    "    fig = go.Figure(data=[go.Bar(x=pd.to_datetime(np.array(prep_us_newcases(d).columns)),\n",
    "                          y=prep_us_newcases(d).iloc[0],\n",
    "                          name=prep_us_newcases(d).index[0]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_newcases(d).columns)),\n",
    "                          y=prep_us_newcases(d).iloc[1],\n",
    "                          name=prep_us_newcases(d).index[1]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_newcases(d).columns)),\n",
    "                          y=prep_us_newcases(d).iloc[2],\n",
    "                          name=prep_us_newcases(d).index[2]),\n",
    "                        go.Bar(x=pd.to_datetime(np.array(prep_us_newcases(d).columns)),\n",
    "                          y=prep_us_newcases(d).iloc[3],\n",
    "                          name=prep_us_newcases(d).index[3])]\n",
    "                         #orientation='h'\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= go.Scattergeo(\n",
    "        lon = prep_county_sum(df3,df4)['Long_'],\n",
    "        lat = prep_county_sum(df3,df4)['Lat'],\n",
    "        text = prep_county_sum(df3,df4)['text'],\n",
    "        hoverinfo = 'text',\n",
    "        marker = dict(\n",
    "                size = 0.05*(prep_county_sum(df3,df4)['Confirmed']),\n",
    "                line_width=0.5,\n",
    "                sizemode = 'area'\n",
    "            ))\n",
    "\n",
    "\n",
    "fig6=go.Figure(data=d)\n",
    "fig6.update_layout(l_map)\n",
    "fig6.update_geos(scope=\"usa\",\n",
    "            lataxis_showgrid=False, \n",
    "            lonaxis_showgrid=False\n",
    "            )\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_7_8(df3,df4, \"Confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_7_8(df3,df4, \"Conf100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_9(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_10(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### UK Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Data Sources**\n",
    "\n",
    " - **Cases**: \n",
    " \n",
    "     - England:   <a href=https://coronavirus.data.gov.uk/ > Public Health England </a>\n",
    "     - Wales:  <a href=https://public.tableau.com/profile/public.health.wales.health.protection#!/vizhome/RapidCOVID-19virology-Public/Headlinesummary > Public Health Wales Coronavirus Dashboard</a>  \n",
    "     - Scotland:  <a href=https://www.gov.scot/coronavirus-covid-19/ > Scottish Government</a> \n",
    " - **Population**: <a href=https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/>  UK Office of National Statistics </a>\n",
    " - **Co-ordinates of local authorities**: <a href= http://geoportal.statistics.gov.uk/> UK Office of National Statistics Open Geography Portal </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data ingestion & transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_6 = \"https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv\"\n",
    "url_7='http://www2.nphs.wales.nhs.uk:8080/CommunitySurveillanceDocs.nsf/61c1e930f9121fd080256f2a004937ed/77fdb9a33544aee88025855100300cab/$FILE/Rapid%20COVID-19%20surveillance%20data.xlsx'\n",
    "url_8='https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/04/trends-in-number-of-people-in-hospital-with-confirmed-or-suspected-covid-19/documents/covid-19-data-by-nhs-board/covid-19-data-by-nhs-board/govscot%3Adocument/HSCA%2B-%2BSG%2BWebsite%2B-%2BIndicator%2BTrends%2Bfor%2Bdaily%2Bdata%2Bpublication%2B-%2BHealth%2BBoard%2BBreakdown.xlsx?forceDownload=true'\n",
    "\n",
    "uk_map=pd.read_csv(\"uk_map.csv\").drop(columns=\"Unnamed: 0\").groupby(by=\"ReportingArea\").mean()\n",
    "uk_pop=pd.read_csv(\"uk_pop.csv\").drop(columns=\"Unnamed: 0\").groupby(by=\"ReportingArea\").sum()\n",
    "\n",
    "def convert(val):\n",
    "    step1=str(val)\n",
    "    step2 = step1.replace(',','')\n",
    "    step3 = step2.replace('**','')\n",
    "    step4= int(step3)\n",
    "    return step4\n",
    "\n",
    "def prep_england_cases(url):\n",
    "    s = requests.get(url).content\n",
    "    england_cases = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "    return england_cases\n",
    "\n",
    "def prep_wales_cases(url):\n",
    "    wales_cases=pd.read_excel (url, 'Tests by specimen date')\n",
    "    return wales_cases\n",
    "\n",
    "def prep_scot_cases(url):\n",
    "    df=pd.read_excel(url, 'Table 1 - Cumulative cases',skiprows=2).replace({'*':0})\n",
    "    df.rename(columns={\n",
    "        'NHS Ayrshire & Arran':'Ayrshire & Arran', 'NHS Borders':'Borders',\n",
    "           'NHS Dumfries & Galloway':'Dumfries and Galloway', 'NHS Fife':'Fife', 'NHS Forth Valley':'Forth Valley',\n",
    "           'NHS Grampian':'Grampian', 'NHS Greater Glasgow & Clyde':'Greater Glasgow and Clyde', 'NHS Highland':'Highland',\n",
    "           'NHS Lanarkshire':'Lanarkshire', 'NHS Lothian':'Lothian', 'NHS Orkney':'Orkney', 'NHS Shetland':'Shetland',\n",
    "           'NHS Tayside':'Tayside', 'NHS Western Isles':'Eileanan Siar (Western Isles)'    \n",
    "    }, inplace=True)\n",
    "    scotland_cases=pd.DataFrame({\n",
    "                       \"ReportingArea\":df.iloc[-1][1:-1].index.values, \n",
    "                       \"Confirmed\": df.iloc[-1][1:-1]})\n",
    "    return scotland_cases\n",
    "\n",
    "def prep_uk_scatter(url1,url2,url3):\n",
    "    d1=prep_england_cases(url1)\n",
    "    d2=prep_wales_cases(url2)\n",
    "    d3=prep_scot_cases(url3)\n",
    "    dd1=d1.drop(d1[d1[\"Area type\"]==\"Region\"].index)\n",
    "    dd1=dd1[[\"Area name\", \"Specimen date\", \"Cumulative lab-confirmed cases\"]][dd1[\"Specimen date\"]==dd1[\"Specimen date\"].max()]\n",
    "    dd1=dd1.drop(labels=[\"Specimen date\"], axis=1)\n",
    "    dd1.rename(columns={\"Area name\":\"ReportingArea\", \"Cumulative lab-confirmed cases\": \"Confirmed\"}, inplace=True)\n",
    "    dd2=d2[d2[\"Specimen date\"]==d2[\"Specimen date\"].max()][[\"Local Authority\", \"Cumulative cases\"]].rename(columns={\"Local Authority\": \"ReportingArea\", \"Cumulative cases\": \"Confirmed\"})\n",
    "    df=dd1.append(dd2, ignore_index=True).append(d3, ignore_index=True)\n",
    "    df=df.astype('str')\n",
    "    df.set_index(\"ReportingArea\", inplace=True)\n",
    "    df=df.merge(uk_pop, right_index=True, left_index=True).merge(uk_map, on=\"ReportingArea\")\n",
    "    df[\"Confirmed\"]=df[\"Confirmed\"].apply(convert)\n",
    "    df[\"Conf100\"]=round(df[\"Confirmed\"]*100000/df[\"All ages\"])\n",
    "    df[\"text\"]= df.index+ ', '+ '<br>Cases: ' + (df['Confirmed']).astype(str)\n",
    "    df[\"text100\"]= df.index + ', '+ '<br>Cases per 100k pop: ' + (round(df['Conf100'],1).astype(str))\n",
    "    return df\n",
    "\n",
    "def prep_eng_trend(url):\n",
    "    df=prep_england_cases(url)\n",
    "    df=df[[\"Area name\", \"Specimen date\", \"Daily lab-confirmed cases\", \"Cumulative lab-confirmed cases\"]]\n",
    "    df=df.rename(columns={\"Area name\": \"ReportingArea\", \"Daily lab-confirmed cases\":\"New cases\", \"Cumulative lab-confirmed cases\": \"Cumulative cases\"})\n",
    "    return df\n",
    "    \n",
    "def prep_wales_trend(url):\n",
    "    df=prep_wales_cases(url)\n",
    "    df=df[[\"Local Authority\", \"Specimen date\", \"Cases (new)\", \"Cumulative cases\"]]\n",
    "    df=df.rename(columns={\"Local Authority\": \"ReportingArea\", \"Cases (new)\":\"New cases\"})\n",
    "    return df\n",
    "\n",
    "def prep_scot_trend(url):\n",
    "    d=df=pd.read_excel(url, 'Table 1 - Cumulative cases',skiprows=2).replace({'*':0})\n",
    "    df=d[[\"Date\", \"Scotland\"]].rename(columns={\"Date\": \"Specimen date\", \"Scotland\": \"Cumulative cases\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#Worst hit local areas\n",
    "def make_fig_12_13(url1,url2,url3,text):\n",
    "    fig = go.Figure(go.Bar(y=prep_uk_scatter(url1,url2, url3).sort_values(by=[text], ascending=True).index[-15:],\n",
    "                          x=prep_uk_scatter(url1,url2, url3).sort_values(by=[text],ascending=True)[text][-15:],\n",
    "                         orientation='h'\n",
    "                        )\n",
    "                 )\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "def make_fig_14(url1,url2,url3):\n",
    "    fig = go.Figure(data=[go.Bar(x=prep_eng_trend(url1)[prep_eng_trend(url1)[\"ReportingArea\"]==\"England\"][\"Specimen date\"],\n",
    "                          y=prep_eng_trend(url1)[prep_eng_trend(url1)[\"ReportingArea\"]==\"England\"][\"Cumulative cases\"],\n",
    "                          name=\"England\"),\n",
    "                        go.Bar(x=prep_wales_trend(url2).groupby(\"Specimen date\").sum().index,\n",
    "                          y=prep_wales_trend(url2).groupby(\"Specimen date\").sum()[\"Cumulative cases\"],\n",
    "                          name=\"Wales\"),\n",
    "                        go.Bar(x=prep_scot_trend(url3)[\"Specimen date\"],\n",
    "                          y=prep_scot_trend(url3)[\"Cumulative cases\"],\n",
    "                          name=\"Scotland\")]\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig\n",
    "\n",
    "def make_fig_15(url1,url2,url3):\n",
    "    fig = go.Figure(data=[go.Bar(x=prep_eng_trend(url1)[prep_eng_trend(url1)[\"ReportingArea\"]==\"England\"][\"Specimen date\"],\n",
    "                          y=prep_eng_trend(url1)[prep_eng_trend(url1)[\"ReportingArea\"]==\"England\"][\"New cases\"],\n",
    "                          name=\"England\"),\n",
    "                        go.Bar(x=prep_wales_trend(url2).groupby(\"Specimen date\").sum().index,\n",
    "                          y=prep_wales_trend(url2).groupby(\"Specimen date\").sum()[\"New cases\"],\n",
    "                          name=\"Wales\"),\n",
    "                        go.Bar(x=prep_scot_trend(url3)[\"Specimen date\"][1:],\n",
    "                          y=prep_scot_trend(url3)[\"Cumulative cases\"][1:].values - prep_scot_trend(url3)[\"Cumulative cases\"][:-1].values,\n",
    "                          name=\"Scotland\")]\n",
    "                        )\n",
    "    fig.update_layout(barmode='stack')\n",
    "    fig.update_layout(l_bar_s)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=go.Scattermapbox(\n",
    "    lon = prep_uk_scatter(url_6,url_7, url_8)['long'],\n",
    "    lat = prep_uk_scatter(url_6,url_7, url_8)['lat'],\n",
    "    text = prep_uk_scatter(url_6,url_7, url_8)['text100'],\n",
    "    hoverinfo = 'text',\n",
    "    marker = dict(\n",
    "            size = 5*np.sqrt(prep_uk_scatter(url_6,url_7, url_8)['Conf100']),\n",
    "            sizemode = 'area',\n",
    "        symbol = 'circle'\n",
    "        ))\n",
    "fig11=go.Figure(data=d)\n",
    "fig11.update_layout(l_mapbox)\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "make_fig_12_13(url_6, url_7, url_8, \"Confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_12_13(url_6, url_7, url_8, \"Conf100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_14(url_6, url_7, url_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_15(url_6, url_7, url_8)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
